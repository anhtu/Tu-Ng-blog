<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

 <title></title>
 <link href="http://anhtu.github.io/atom.xml" rel="self"/>
 <link href="http://anhtu.github.io/"/>
 <updated>2016-11-06T20:49:33-08:00</updated>
 <id>http://anhtu.github.io</id>
 <author>
   <name>Tu Nguyen</name>
   <email>anhtu2687@gmail.com</email>
 </author>

 
 <entry>
   <title>Lessons learned from the Otto Product Classification Challenge</title>
   <link href="http://anhtu.github.io/2015/06/01/lessons-learned-from-the-otto-product-classification-challenge/"/>
   <updated>2015-06-01T00:00:00-07:00</updated>
   <id>http://anhtu.github.io/2015/06/01/lessons-learned-from-the-otto-product-classification-challenge</id>
   <content type="html">&lt;p&gt;Otto Group Product Classification Challenge is the most popular Kaggle competition ever (in terms of participants). The task was to build a predictive model to classify products into one of the nine categories. I was fortunate to take part in this competition and learned a great deal of techniques from other fellow Kagglers.&lt;/p&gt;

&lt;p&gt;One of the challenges in this competition is that the data was obfuscated, which made feature engineering impossible. We were given a total of 93 numerical features, with a huge 200,000 observations in both training and test data. The fun part of this challenge was that it all came down to understanding about the machine learning algorithms and hyper-parameter optimization.&lt;/p&gt;

&lt;p&gt;The final result was tight. I joined the competition very late, only had 2 weeks to work on it but Iâ€™m quite satisfied with the result. I ended up in 91th place (top 3%) with a logloss of 0.41576, which was not bad given a huge number of competitors (3514 teams).&lt;/p&gt;

&lt;p&gt;My final model is a linear stacking&lt;/p&gt;
</content>
 </entry>
 

</feed>
